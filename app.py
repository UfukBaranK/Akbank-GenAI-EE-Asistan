import os
import streamlit as st
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
    
# Proje GeliÅŸtirme OrtamÄ± (Kriter 1)
# .env dosyasÄ±ndan API anahtarÄ±nÄ± yÃ¼kler (Yerel Ã§alÄ±ÅŸma iÃ§in kritik)
load_dotenv()

# KRÄ°TÄ°K BÃ–LÃœM: API AnahtarÄ±nÄ± Ortam DeÄŸiÅŸkeni Olarak Garantileme (Ã‡Ã¶zÃ¼m Mimarisi Kriteri)
# LangChain'in ve Google SDK'nÄ±n API anahtarÄ±nÄ± doÄŸru okumasÄ±nÄ± garanti eder.
API_KEY_VALUE = os.getenv("GEMINI_API_KEY")
if not API_KEY_VALUE:
    API_KEY_VALUE = os.getenv("GOOGLE_API_KEY") # Ä°kinci kontrol
    
if API_KEY_VALUE:
    os.environ["GEMINI_API_KEY"] = API_KEY_VALUE
    os.environ["GOOGLE_API_KEY"] = API_KEY_VALUE # Ortam deÄŸiÅŸkenini set et
else:
    print("ğŸš¨ HATA: .env dosyasÄ±nda geÃ§erli bir API AnahtarÄ± bulunamadÄ±.")
    
# -- Sabitler (Ã‡Ã¶zÃ¼m Mimarisi Kriteri) --
VECTOR_DB_PATH = "./vector_db"  # VektÃ¶r VeritabanÄ± (ChromaDB) yerel depolama yolu
GEMINI_MODEL = "gemini-2.5-flash" # HÄ±z ve maliyet etkinliÄŸi iÃ§in seÃ§ilen LLM
EMBEDDING_MODEL = "models/embedding-001" # Google'Ä±n yÃ¼ksek performanslÄ± embedding modeli
    
def setup_rag_chain():
    # 1. Kimlik DoÄŸrulama KontrolÃ¼ (Ã‡alÄ±ÅŸma KÄ±lavuzu Kriteri)
    if not os.environ.get("GEMINI_API_KEY") and not os.environ.get("GOOGLE_API_KEY"):
        st.error("ğŸš¨ HATA: API AnahtarÄ± bulunamadÄ±. LÃ¼tfen Streamlit Secrets'Ä±/Local .env dosyasÄ±nÄ± kontrol edin.")
        return None
        
    # 2. Veri Seti KontrolÃ¼ (Veri Seti HazÄ±rlama Kriteri)
    # BÃ¼yÃ¼k veri seti (381 kaynak) bulutta olmadÄ±ÄŸÄ± iÃ§in, sadece yerel ortamda RAG kurulabilir.
    if not os.path.exists(VECTOR_DB_PATH):
        st.error(f"ğŸš¨ HATA: VeritabanÄ± (vector_db klasÃ¶rÃ¼) bulut ortamÄ±nda mevcut deÄŸil.")
        st.warning("âš ï¸ UygulamanÄ±n **381 KaynaklÄ±k RAG Kabiliyetini** test etmek iÃ§in, lÃ¼tfen README.md'deki kÄ±lavuza uyarak projeyi yerel bilgisayarÄ±nÄ±zda Ã§alÄ±ÅŸtÄ±rÄ±n.")
        return None
    
    # 3. RAG Zinciri Kurulumu (Ã‡Ã¶zÃ¼m Mimarisi Kriteri)
    
    # 3.1 VektÃ¶r VeritabanÄ±nÄ± YÃ¼kleme (Retrieval)
    # ChromaDB (VektÃ¶r Database) yerel dosyadan geri yÃ¼kleniyor.
    embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL)
    vectorstore = Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embeddings)
    
    # Retriever (AlÄ±cÄ±): BÃ¼yÃ¼k veri seti nedeniyle kapsayÄ±cÄ±lÄ±ÄŸÄ± artÄ±rmak iÃ§in k=5 seÃ§ildi.
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5}) 
    
    # 3.2 LLM (Generation) ve Prompt HazÄ±rlÄ±ÄŸÄ±
    llm = ChatGoogleGenerativeAI(model=GEMINI_MODEL, temperature=0.1) 

    # ESNEK PROMPT MÃœHENDÄ°SLÄ°ÄÄ° (Kriter 4: Elde Edilen SonuÃ§lar)
    # RAG baÅŸarÄ±sÄ±z olsa bile (baÄŸlam yetersizse), LLM'in genel mÃ¼hendislik bilgisiyle cevap vermesini saÄŸlar.
    template = """Sen, Elektrik-Elektronik MÃ¼hendisliÄŸi Ã¶ÄŸrencilerine yÃ¶nelik pratik bir ders asistanÄ±sÄ±n. 
    GÃ¶revin, kullanÄ±cÄ±nÄ±n sorusuna **enerjik ve profesyonel** bir tonda **detaylÄ± TÃœRKÃ‡E cevap** vermektir.

    **KURAL:**
    1. **Ã–ncelikli olarak** aÅŸaÄŸÄ±daki 'BaÄŸlam' kÄ±smÄ±nda bulunan bilgileri kullan.
    2. EÄŸer 'BaÄŸlam'da soruya dair **yeterli veya net bir cevap yoksa**, genel mÃ¼hendislik bilgini ve temel kavramlarÄ± kullanarak soruyu yanÄ±tla.
    3. CevabÄ±nÄ± her zaman profesyonel ve yapÄ±landÄ±rÄ±lmÄ±ÅŸ bir dille sun.

    BaÄŸlam: 
    {context}

    Soru: {question}

    TÃœRKÃ‡E Cevap:"""
    prompt = ChatPromptTemplate.from_template(template)
    
    # 3.3 LangChain Expression Language (LCEL) ile Zincir OluÅŸturma
    # TÃ¼m RAG adÄ±mlarÄ±nÄ± modÃ¼ler ve bakÄ±mÄ± kolay bir zincirde birleÅŸtirir.
    rag_chain = (
        # 1. BaÄŸlamÄ± retriever'dan al ve 2. Soruyu doÄŸrudan geÃ§ir.
        {"context": retriever | (lambda x: "\n\n".join([doc.page_content for doc in x])), 
         "question": RunnablePassthrough()} 
        | prompt # 3. Prompt'u uygula
        | llm # 4. LLM'den cevabÄ± al
        | StrOutputParser() # 5. CevabÄ± metin olarak dÃ¶ndÃ¼r.
    )
    return rag_chain
    
# Streamlit Ana Fonksiyon (Kriter 5: Web ArayÃ¼zÃ¼)
def main():
    st.set_page_config(page_title="EE Ders AsistanÄ±", layout="wide")
    
    # GÃœNCELLENMÄ°Å BAÅLIK VE AÃ‡IKLAMA (Proje BaÅŸlÄ±ÄŸÄ± Kriteri)
    st.title("ğŸ’¡ Elektrik-Elektronik MÃ¼hendisliÄŸi AsistanÄ± (RAG)")
    st.caption("ğŸ” Hacettepe EE Ders KitaplarÄ± ve YardÄ±mcÄ± Kaynaklara DayalÄ±, Gemini Destekli AkÄ±llÄ± Asistan.")
    
    if 'rag_chain' not in st.session_state:
        st.session_state.rag_chain = setup_rag_chain()
        if st.session_state.rag_chain is None:
            return 
    
    user_question = st.text_input("EE Ders KitaplarÄ±na dair teknik bir soru sor:", 
                                  placeholder="Ã–rn: 'ELE489 dersindeki L1 ve L2 regÃ¼larizasyon teknikleri arasÄ±ndaki farklar nelerdir?'")
    
    if user_question:
        with st.spinner("ğŸš€ Bilgi KaynaklarÄ±n TaranÄ±yor..."):
            try:
                response = st.session_state.rag_chain.invoke(user_question)
                st.success("Cevap HazÄ±r!")
                st.markdown(f"**Asistan YanÄ±tÄ±:** \n\n {response}")
            except Exception as e:
                st.error(f"API Ã‡aÄŸrÄ±sÄ±nda Hata Kodu: {e}")
                st.warning("Bu hatayÄ± yerel ortamda alÄ±yorsanÄ±z: GÃ¼nlÃ¼k kota dolmuÅŸ olabilir veya API anahtarÄ±nÄ±z Google Cloud'da kÄ±sÄ±tlanmÄ±ÅŸtÄ±r.")
    
if __name__ == "__main__":
    main()